{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OneShotDL Prototype\n",
    "\n",
    "This notebook shows the general experiment setup and code architecture that should be used within the project. It serves as a blueprint for the more advanced models and settings that we aim to develop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pySOT import SyncStrategyNoConstraints, LatinHypercube, RBFInterpolant, CandidateDYCORS, CubicKernel, LinearTail\n",
    "import csv\n",
    "from threading import Thread, current_thread\n",
    "from datetime import datetime\n",
    "from poap.controller import ThreadController, BasicWorkerThread, SerialController\n",
    "import os.path\n",
    "import time\n",
    "import h5py\n",
    "\n",
    "# use keras for the cnn tuning example\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.models import load_model\n",
    "from keras.layers import Dense, Dropout, Flatten, Input\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.losses import categorical_crossentropy\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import backend as K\n",
    "\n",
    "# helper classes of OneShotDL\n",
    "from helpers import load_mnist, split_and_select_random_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Minimal example of using pySOT to implement HORD\n",
    "\n",
    "We aim to use the method proposed in <i>Efficient Hyperparameter Optimization of Deep Learning Algorithms Using\n",
    "Deterministic RBF Surrogates</i> (Ilievski et al., 2017) to tune the models. The paper shows it outperforms popular Bayesian Optimization approaches like SMAC and TPE, especially when the number of parameters to tune is large.\n",
    "\n",
    "The paper: https://arxiv.org/pdf/1607.08316.pdf \n",
    "\n",
    "Their implementation uses the pySOT library. A minimal example of how to use that package is shown below. It requires a class to be written with an objective function and allowed ranges for parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Test():\n",
    "    \n",
    "    def __init__(self, dim=2):\n",
    "        # these attributes are required by pySOT\n",
    "        self.xlow = np.array([-2, 2])\n",
    "        self.xup = np.array([-0.01, 3.1])\n",
    "        self.continuous = np.arange(0,dim)\n",
    "        self.integer = np.array([])\n",
    "        self.dim = dim\n",
    "    \n",
    "        self.counter = 0\n",
    "        \n",
    "    def objfunction(self, x):\n",
    "        # a random objective function for illustration \n",
    "        # optimal solution given the ranges is 0 at x = [-2, 2]\n",
    "        self.counter += 1\n",
    "        score = 2 - np.square(x[0]) + x[1]\n",
    "        print(\"Experiment {}. Params: {}. Score: {}.\".format(self.counter, x, score))\n",
    "        return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use multi threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = Test()\n",
    "maxeval = 100\n",
    "nsamples = 1\n",
    "nthreads = 1\n",
    "\n",
    "# Create a strategy and a controller\n",
    "controller = ThreadController()\n",
    "controller.strategy = SyncStrategyNoConstraints(worker_id=0, \n",
    "                                                data=data,\n",
    "                                                maxeval=maxeval, \n",
    "                                                nsamples=nsamples,\n",
    "                                                exp_design=LatinHypercube(dim=data.dim, npts=2*(data.dim+1)),\n",
    "                                                response_surface=RBFInterpolant(kernel=CubicKernel, maxp=maxeval),\n",
    "                                                sampling_method=CandidateDYCORS(data=data, numcand=100*data.dim))\n",
    "\n",
    "# Launch the threads and give them access to the objective function\n",
    "for _ in range(nthreads):\n",
    "    worker = BasicWorkerThread(controller, data.objfunction)\n",
    "    controller.launch_worker(worker)\n",
    "\n",
    "# Run the optimization strategy\n",
    "result = controller.run()\n",
    "\n",
    "print('Best value found: {0}'.format(result.value))\n",
    "print('Best solution found: {0}\\n'.format(\n",
    "    np.array_str(result.params[0], max_line_width=np.inf,\n",
    "                 precision=5, suppress_small=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Prototype code for tuning a One Shot CNN on Fashion MNIST\n",
    "\n",
    "To use this approach for tuning Deep Learning models, we need classes that train and evaluate different architectures of Neural Networks. This prototype shows how such a class could look. It trains a simple CNN on five randomly chosen images of five randomly chosen classes and evaluates the found model on the test images of those same five classes. This process is repeated a number of times for every combination of parameter values to achieve a form of cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneShotPrototype():\n",
    "    \n",
    "    def __init__(self, dim=5):\n",
    "        \n",
    "        self.hyperparams = ['num_conv_layers', 'num_dense_layers', 'neurons_conv', 'neurons_dense', 'dropout_rate']\n",
    "        self.hyper_map = {self.hyperparams[i]:i for i in range(len(self.hyperparams))}\n",
    "        \n",
    "        # this may need a more intuitive structure\n",
    "        self.xlow = np.array([1, 1, 8, 8, 0.0])\n",
    "        self.xup = np.array([4, 4, 64, 128, 0.75])\n",
    "        self.continuous = np.arange(4,dim)\n",
    "        self.integer = np.arange(0,4)\n",
    "        self.dim = dim\n",
    "        \n",
    "        # fixed parameters\n",
    "        self.batchsize = 128\n",
    "        self.epochs = 200\n",
    "        self.nfolds = 5 # for cross validation\n",
    "        \n",
    "        # data\n",
    "        self.x_train, self.y_train = load_mnist(\"./Data/\", kind='train')\n",
    "        self.x_test, self.y_test = load_mnist(\"./Data/\", kind='test')\n",
    "        self.num_classes = self.y_test.shape[1]\n",
    "        \n",
    "        # logging results\n",
    "        #self.param_log = np.empty(shape=(,dim))\n",
    "        #self.scores_log = np.empty(shape=(,self.nfolds))\n",
    "        \n",
    "        # counter\n",
    "        self.exp_number = 0\n",
    "\n",
    "\n",
    "    def objfunction(self, params):\n",
    "        \"\"\" The overall objective function to provide to pySOT's black box optimization. \"\"\"\n",
    "        \n",
    "        self.exp_number += 1\n",
    "        print(\"--------------\\nExperiment {}.\\n--------------\".format(self.exp_number))\n",
    "        \n",
    "        def define_model(params):\n",
    "            \"\"\" Creates the Keras model based on given parameters. \"\"\"\n",
    "\n",
    "            model = Sequential()\n",
    "            \n",
    "            # add first convolutional layer and specify input shape\n",
    "            model.add(Conv2D(int(params[self.hyper_map['neurons_conv']]), \n",
    "                             kernel_size=(3,3), activation='relu', \n",
    "                             input_shape=(28,28,1), data_format=\"channels_last\"))\n",
    "            \n",
    "            # possibly add more\n",
    "            if int(params[self.hyper_map['num_conv_layers']]) > 1:\n",
    "                for l in range(1,int(params[0])):\n",
    "                    model.add(Conv2D(int(params[self.hyper_map['neurons_conv']]), (3, 3), activation='relu'))\n",
    "            \n",
    "            # max pool > dropout > flatten\n",
    "            model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "            model.add(Dropout(params[self.hyper_map['dropout_rate']]))\n",
    "            model.add(Flatten())\n",
    "            \n",
    "            # add dense layers before the classification layer\n",
    "            for l in range(int(params[self.hyper_map['num_dense_layers']])):\n",
    "                model.add(Dense(int(params[self.hyper_map['neurons_dense']]), activation='relu'))\n",
    "            \n",
    "            # classification layer\n",
    "            model.add(Dense(self.num_classes, activation='softmax'))\n",
    "            \n",
    "            # compile and return\n",
    "            model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                          optimizer='rmsprop',\n",
    "                          metrics=['accuracy'])\n",
    "            \n",
    "            # create data generator, later including augmentations\n",
    "            datagen = ImageDataGenerator()\n",
    "\n",
    "            return model, datagen\n",
    "            \n",
    "\n",
    "        def cross_validate(x, y, xtest, ytest, params, n):\n",
    "            \"\"\" Cross validate with random sampling. \"\"\"\n",
    "            \n",
    "            print(\"Cross validating..\")\n",
    "            scores = []\n",
    "            for i in range(n):\n",
    "                x_target_labeled, y_target, x_test, y_test, _, _, _ = \\\n",
    "                    split_and_select_random_data(x, y, xtest, ytest,\n",
    "                                                 num_target_classes=5, num_examples_per_class=1)\n",
    "                model, datagen = define_model(params)\n",
    "                print(\"fit {}:\".format(i+1))\n",
    "                # fits the model on batches with real-time data augmentation:\n",
    "                model.fit_generator(datagen.flow(x_target_labeled, y_target, batch_size=x_target_labeled.shape[0]), \n",
    "                                    steps_per_epoch=1, epochs=self.epochs, verbose=0)\n",
    "\n",
    "                loss, accuracy = model.evaluate(x_test, y_test, verbose=0, batch_size=y.shape[0])\n",
    "                print(\"test accuracy: {}%.\".format(round(accuracy*100, 2)))\n",
    "                scores.append(accuracy)\n",
    "            \n",
    "            return scores\n",
    "        \n",
    "        \n",
    "        print(\"params: {}.\".format(params))\n",
    "        scores = cross_validate(self.x_train, self.y_train, self.x_test, self.y_test, params, self.nfolds)\n",
    "        print(\"Scores: {}.\\nMean: {}%. Standard deviation: {}%\".format(scores, round(np.mean(scores)*100, 2), round(np.std(scores)*100, 2)))\n",
    "        \n",
    "        # to minimize, return this to pySOT\n",
    "        return -np.mean(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Testing the prototype class\n",
    "Test the prototype class for a single set of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = OneShotPrototype()\n",
    "score = test.objfunction([2, 2, 16, 16, 0.5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try tuning the model with HORD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = OneShotPrototype()\n",
    "maxeval = 15\n",
    "nsamples = 1 # one experiment at the time\n",
    "\n",
    "# create the controller\n",
    "controller = SerialController(data.objfunction)\n",
    "# experiment design\n",
    "exp_des = LatinHypercube(dim=data.dim, npts=2*data.dim+1)\n",
    "# Use a cubic RBF interpolant with a linear tail\n",
    "surrogate = RBFInterpolant(kernel=CubicKernel, tail=LinearTail, maxp=maxeval)\n",
    "# Use DYCORS with 100d candidate points\n",
    "adapt_samp = CandidateDYCORS(data=data, numcand=100*data.dim)\n",
    "\n",
    "strategy = SyncStrategyNoConstraints(worker_id=0, data=data, maxeval=maxeval, nsamples=1,\n",
    "                                     exp_design=exp_des, response_surface=surrogate,\n",
    "                                     sampling_method=adapt_samp)\n",
    "controller.strategy = strategy\n",
    "\n",
    "# Run the optimization strategy\n",
    "start_time = datetime.now()\n",
    "result = controller.run()\n",
    "\n",
    "print('Best value found: {0}'.format(result.value))\n",
    "print('Best solution found: {0}\\n'.format(\n",
    "    np.array_str(result.params[0], max_line_width=np.inf,\n",
    "                 precision=5, suppress_small=True)))\n",
    "\n",
    "millis = int(round(time.time() * 1000))\n",
    "print('Started: '+str(start_time)+'. Ended: ' + str(datetime.now()) + ' (' + str(millis) + ')')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that using the multi-thread controller does not work for this class, because this conflicts with Keras, which already uses all available cores."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
